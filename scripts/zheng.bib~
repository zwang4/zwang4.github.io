@inproceedings{iccpol,
title = "A convolution BiLSTM neural network model for Chinese event extraction",
author = "Ying Zeng and Honghui Yang and Yansong Feng and Zheng Wang and Dongyan Zhao",
note = "The final publication is available at Springer via http://dx.doi.org/10.1007/978-3-319-50496-4_23",
year = "2016",
month = "12",
doi = "10.1007/978-3-319-50496-4_23",
isbn = "9783319504957",
series = "Lecture Notes in Computer Science",
publisher = "Springer",
pages = "275--287",
editor = "Chin-Yew Lin and Nianwen Xue and Dongyan Zhao and Xuanjing Huang and Yansong Feng",
booktitle = "24th International Conference on Computer Processing of Oriental Languages (ICCPOL)",
}

@inproceedings{iccpol2,
title = "Improving first order temporal fact extraction with unreliable data",
author = "Bingfeng Luo and Yansong Feng and Zheng Wang and Dongyan Zhao",
note = "The final publication is available at Springer via http://dx.doi.org/10.1007/978-3-319-50496-4_21",
year = "2016",
month = "12",
doi = "10.1007/978-3-319-50496-4_21",
isbn = "9783319504957",
series = "Lecture Notes in Computer Science",
publisher = "Springer",
pages = "251--262",
editor = "Chin-Yew Lin and Nianwen Xue and Dongyan Zhao and Xuanjing Huang and Yansong Feng",
booktitle = "24th International Conference on Computer Processing of Oriental Languages (ICCPOL)",
}

@inproceedings{smartx16,
title = "DRET: a system for detecting evil-twin attacks in smart homes",
abstract = "Evil-twin is one of most commonly attacks in the WIFI environments, with which an attacker can steal sensitive information by cloning a fake AP in Smart Homes. The current approaches of detecting Evil-twin AP uses some identities/fingerprints of legitimated APs to identify rouge APs. Prior work in the area uses information like SSIDs, MAC addresses, and network traffics to detect bogus APs. However, such information can be easily intimated by the attacker, leading to low detection rates. This paper introduces a novel Evil-Twin AP detection method based on received signal strength indicator (RSSI). Our approach exploits the fact that the AP location is relatively stable in Smart Homes, which is to great extent to meet the requirement of the detection factor not easy to imitate as previous refer. We employ two detection strategies: a single position detection and a multi-positioned detection methods. Our approach exploits the multipath effect of WIFI signals to translate the problem of attack detection into AP positioning detection. Compared to classical detection methods, our approach can perform detection without relying on professional devices. Experimental results show that the single position detection approach achieves 20 seconds¡¯ reduction of delay time with an accuracy of 98%, whereas the multi-positioned detection approach achieves 90% correct.",
keywords = "Smart Homes, Evil-Twin Attack, RSSI, Detection",
author = "Zhanyong Tang and Yujie Zhao and Lei Yang and Shengde Qi and Dingyi Fang and Xiaojiang Chen and Xiaoqing Gong and Zheng Wang",
year = "2016",
month = "7",
booktitle = "The 2016 International Conference on Smart X"
}

@inproceedings{icpads15,
title = "Power capping: what works, what does not",
keywords = "Power optimization, Power capping, Compiler, DVFS, RAPL",
author = "Pavlos Petoumenos and Lev Mukhanov and Zheng Wang and Hugh Leather and Dimitrios Nikolopoulos",
note = "?2015 IEEE. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE.",
year = "2015",
month = "12",
doi = "10.1109/ICPADS.2015.72",
isbn = "9780769557854",
pages = "525--534",
booktitle = "IEEE 21st International Conference on Parallel and Distributed Systems (ICPADS)",
publisher = "IEEE",
}


@inproceedings{cpc15,
title = "Intelligent heuristic construction with active learning",
author = "William Ogilvie and Zheng Wang and Pavlos Petoumenos and Hugh Leather",
year = "2015",
month = "1",
booktitle = "Compilers for Parallel Computing (CPC)",
}

@inproceedings{hipeac11,
title = "A workload-aware mapping approach for data-parallel programs",
abstract = "Much compiler-orientated work in the area of mapping parallel programs to parallel architectures has ignored the issue of external workload. Given that the majority of platforms will not be dedicated to just one task at a time, the impact of other jobs needs to be addressed. As mapping is highly dependent on the underlying machine, a technique that is easily portable across platforms is also desirable.In this paper we develop an approach for predicting the optimal number of threads for a given data-parallel application in the presence of external workload. We achieve 93.7% of the maximum speedup available which gives an average speedup of 1.66 on 4 cores, a factor 1.24 times better than the OpenMP compiler's default policy. We also develop an alternative cooperative model that minimizes the impact on external workload while still giving an improved average speedup. Finally, we evaluate our approach on a separate 8-core machine giving an average 1.33 times speedup over the default policy showing the portability of our approach.",
author = "Dominik Grewe and Zheng Wang and Michael O'Boyle",
year = "2011",
doi = "10.1145/1944862.1944881",
isbn = "9781450302418",
pages = "117--126",
booktitle = "HiPEAC '11 Proceedings of the 6th International Conference on High Performance and Embedded Architectures and Compilers",
publisher = "ACM",
}


@inproceedings{lcpc14,
title = "Fast automatic heuristic construction using active learning",
abstract = "Building effective optimization heuristics is a challenging task which often takes developers several months if not years to complete. Predictive modelling has recently emerged as a promising solution, automatically constructing heuristics from training data. However, obtaining this data can take months per platform. This is becoming an ever more critical problem and if no solution is found we shall be left with out of date heuristics which cannot extract the best performance from modern machines.In this work, we present a low-cost predictive modelling approach for automatic heuristic construction which significantly reduces this training overhead. Typically in supervised learning the training instances are randomly selected to evaluate regardless of how much useful information they carry. This wastes effort on parts of the space that contribute little to the quality of the produced heuristic. Our approach, on the other hand, uses active learning to select and only focus on the most useful training examples.We demonstrate this technique by automatically constructing a model to determine on which device to execute four parallel programs at differing problem dimensions for a representative Cpu¨CGpu based heterogeneous system. Our methodology is remarkably simple and yet effective, making it a strong candidate for wide adoption. At high levels of classification accuracy the average learning speed-up is 3x, as compared to the state-of-the-art.",
keywords = "Machine learning, Workload scheduling",
author = "William Ogilvie and Pavlos Petoumenos and Zheng Wang and Hugh Leather",
year = "2014",
month = "9",
doi = "10.1007/978-3-319-17473-0_10",
isbn = "9783319174723",
series = "Lecture Notes in Computer Science",
publisher = "Springer",
pages = "146--160",
editor = "James Brodman and Peng Tu",
booktitle = "Languages and Compilers for Parallel Computing",
}

@inproceedings{pact14,
title = "Active learning accelerated automatic heuristic construction for parallel program mapping",
abstract = "Building effective optimization heuristics is a challenging task which often takes developers several months if not years to complete. Predictive modelling has recently emerged as a promising solution, automatically constructing heuristics from training data, however, obtaining this data can take months per platform. This is becoming an ever more critical problem as the pace of change in architecture increases. Indeed, if no solution is found we shall be left with out of date heuristics which cannot extract the best performance from modern machines.In this work, we present a low-cost predictive modelling approach for automatic heuristic construction which significantly reduces this training overhead. Typically in supervised learning the training instances are randomly selected to evaluate regardless of how much useful information they carry, but this wastes effort on parts of the space that contribute little to the quality of the produced heuristic. Our approach, on the other hand, uses active learning to select and only focus on the most useful training examples and thus reduces the training overhead.We demonstrate this technique by automatically creating a model to determine on which device to execute four parallel programs at differing problem dimensions for a representative CPU-GPU based system. Our methodology is remarkably simple and yet effective, making it a strong candidate for wide adoption. At high levels of classification accuracy the average learning speed-up is 3x, as compared to the state-of-the-art.",
author = "William Ogilvie and Pavlos Petoumenos and Zheng Wang and Hugh Leather",
year = "2014",
doi = "10.1145/2628071.2628128",
isbn = "9781450328098",
pages = "481--482",
booktitle = "The 23rd international conference on Parallel architectures and compilation (PACT)",
publisher = "ACM",
}


@inproceedings{cc14,
title = "Exploitation of GPUs for the parallelisation of probably parallel legacy code",
abstract = "General purpose Gpus provide massive compute power, but are notoriously difficult to program. In this paper we present a complete compilation strategy to exploit Gpus for the parallelisation of sequential legacy code. Using hybrid data dependence analysis combining static and dynamic information, our compiler automatically detects suitable parallelism and generates parallel OpenCl code from sequential programs. We exploit the fact that dependence profiling provides us with parallel loop candidates that are highly likely to be genuinely parallel, but cannot be statically proven so. For the efficient Gpu parallelisation of those probably parallel loop candidates, we propose a novel software speculation scheme, which ensures correctness for the unlikely, yet possible case of dynamically detected dependence violations. Our scheme operates in place and supports speculative read and write operations. We demonstrate the effectiveness of our approach in detecting and exploiting parallelism using sequential codes from the Nas benchmark suite. We achieve an average speedup of 3.2x, and up to 99x, over the sequential baseline. On average, this is 1.42 times faster than state-of-the-art speculation schemes and corresponds to 99% of the performance level of a manual Gpu implementation developed by independent expert programmers.",
keywords = "GPU, OpenCL, Parallelization, Thread Level Speculation",
author = "Zheng Wang and Daniel Powell and Bjorn Franke and Michael O'Boyle",
year = "2014",
doi = "10.1007/978-3-642-54807-9_9",
isbn = "9783642548062",
series = "Lecture Notes in Computer Science",
publisher = "Springer Verlag",
pages = "154--173",
editor = "Albert Cohen",
booktitle = " 23rd International Conference on Compiler Construction (CC)",
}

@inproceedings{lcpc14,
title = "OpenCL task partitioning in the presence of GPU contention",
abstract = "Heterogeneous multi- and many-core systems are increasingly prevalent in the desktop and mobile domains. On these systems it is common for programs to compete with co-running programs for resources. While multi-task scheduling for CPUs is a well-studied area, how to partitioning and map computing tasks onto the heterogeneous system in the presence of GPU contention (i.e. multiple programs compete for the GPU) remains an outstanding problem.In this paper we consider the problem of partitioning OpenCL kernels on a CPU-GPU based system in the presence of contention on the GPU. We propose a machine learning-based approach that predicts the optimal partitioning of OpenCL kernels, explicitly taking GPU contention into account. Our predictive model achieves a speed-up of 1.92 over a scheme that always uses the GPU. When compared to two state-of-the-art dynamic approaches our model achieves speed-ups of 1.54 and 2.56 respectively.",
author = "Dominik Grewe and Zheng Wang and Michael O'Boyle",
year = "2014",
doi = "10.1007/978-3-319-09967-5_5",
isbn = "9783319099668",
series = "Lecture Notes in Computer Science",
publisher = "Springer",
pages = "87--101",
booktitle = " The 26th International Workshop on Languages and Compilers for Parallel Computing (LCPC)",
}

@inproceedings{hipc14,
title = "Smart multi-task scheduling for OpenCL programs on CPU/GPU heterogeneous platforms",
abstract = "Heterogeneous systems consisting of multiple CPUs and GPUs are increasingly attractive as platforms for high performance computing. Such platforms are usually programmed using OpenCL which provides program portability by allowing the same program to execute on different types of device. As such systems become more mainstream, they will move from application dedicated devices to platforms that need to support multiple concurrent user applications. Here there is a need to determine when and where to map different applications so as to best utilize the available heterogeneous hardware resources. In this paper, we present an efficient OpenCL task scheduling scheme which schedules multiple kernels from multiple programs on CPU/GPU heterogeneous platforms. It does this by determining at runtime which kernels are likely to best utilize a device. We show that speedup is a good scheduling priority function and develop a novel model that predicts a kernel's speedup based on its static code structure. Our scheduler uses this prediction and runtime input data size to prioritize and schedule tasks. This technique is applied to a large set of concurrent OpenCL kernels. We evaluated our approach for system throughput and average turn-around time against competitive techniques on two different platforms: a Core i7/Nvidia GTX590 and a Core i7/AMD Tahiti 7970 platforms. For system throughput, we achieve, on average, a 1.21x and 1.25x improvement over the best competitors on the NVIDIA and AMD platforms respectively. Our approach reduces the turnaround time, on average, by at least 1.5x and 1.2x on the NVIDIA and AMD platforms respectively, when compared to alternative approaches.",
author = "Yuan Wen and Zheng Wang and Michael O'Boyle",
year = "2014",
doi = "10.1109/HiPC.2014.7116910",
isbn = "9781479959754",
booktitle = "21st Annual IEEE International Conference on High Performance Computing (HiPC)",
publisher = "IEEE",
}


@inproceedings{cgo13-1,
title = "Smart, adaptive mapping of parallelism in the presence of external workload",
abstract = "Given the wide scale adoption of multi-cores in main stream computing, parallel programs rarely execute in isolation and have to share the platform with other applications that compete for resources. If the external workload is not considered when mapping a program, it leads to a significant drop in performance. This paper describes an automatic approach that combines compile-time knowledge of the program with dynamic runtime workload information to determine the best adaptive mapping of programs to available resources. This approach delivers increased performance for the target application without penalizing the existing workload. This approach is evaluated on NAS and SpecOMP parallel bench-mark programs across a wide range of workload scenarios. On average, our approach achieves performance gain of 1.5¡Á over a state-of-art scheme on a 12 core machine.",
author = "Emani, {Murali Krishna} and Zheng Wang and Michael O'Boyle",
year = "2013",
month = "2",
doi = "10.1109/CGO.2013.6495010",
isbn = "9781467355247",
pages = "1--10",
booktitle = "IEEE/ACM International Symposium on Code Generation and Optimization (CGO)",
publisher = "IEEE",
}


@inproceedings{cgo13-2,
title = "Portable mapping of data parallel programs to OpenCL for heterogeneous systems",
keywords = "application program interfaces, graphics processing units, multiprocessing systems, parallel programming, program compilers, Core i7/AMD Radeon 7970, Core i7/NVIDIA GeForce GTX 580, NAS parallel benchmark suite, OpenCL code, OpenMP code, compiler, data parallel program mapping, data transformations, data-parallel OpenMP programs, general purpose GPU based systems, heterogeneous multicores, heterogeneous systems, high level language, predictive modeling, GPU, OpenCL, Machine, Learning Mapping",
author = "D. Grewe and Zheng Wang and M.F.P. O'Boyle",
year = "2013",
doi = "10.1109/CGO.2013.6494993",
isbn = "978-1-4673-5524-7",
pages = "1--10",
booktitle = "IEEE/ACM International Symposium on Code Generation and Optimization (CGO)",
publisher = "IEEE",
}


@inproceedings{pact10,
title = "Partitioning streaming parallelism for multi-cores: a machine learning based approach",
abstract = "Stream based languages are a popular approach to expressing parallelism in modern applications. The efficient mapping of streaming parallelism to multi-core processors is, however, highly dependent on the program and underlying architecture. We address this by developing a portable and automatic compiler-based approach to partitioning streaming programs using machine learning. Our technique predicts the ideal partition structure for a given streaming application using prior knowledge learned off-line. Using the predictor we rapidly search the program space (without executing any code) to generate and select a good partition. We applied this technique to standard StreamIt applications and compared against existing approaches. On a 4-core platform, our approach achieves 60% of the best performance found by iteratively compiling and executing over 3000 different partitions per program. We obtain, on average, a 1.90x speedup over the already tuned partitioning scheme of the StreamIt compiler. When compared against a state-of-the-art analytical, model-based approach, we achieve, on average, a 1.77x performance improvement. By porting our approach to a 8-core platform, we are able to obtain 1.8x improvement over the StreamIt default scheme, demonstrating the portability of our approach.",
keywords = "compiler optimization, machine learning, partitioning streaming parallelism",
author = "Zheng Wang and O'Boyle, {Michael F.P.}",
year = "2010",
doi = "10.1145/1854273.1854313",
isbn = "978-1-4503-0178-7",
pages = "307--318",
booktitle = "The 19th International Conference on Parallel Architectures and Compilation Techniques (PACT 2010)",
publisher = "ACM",
}

@misc{patent1,
title = "Method and Compiler of Compiling a Program",
author = "Wenjun Wang and Zheng Wang and Xin Zhong",
year = "2009",
type = "Patent",
}

@misc{patent2,
title = "Method of Managing Data Movement and Cell Broadband Engine Processor Using the Same SPE",
author = "Zheng Wang and Liang Chen and Wenjun Wang",
year = "2009",
type = "Patent",
}

@misc{patent3,
title = "Overlay Instruction Accessing Unit and Overlay Instruction Accessing Method",
author = "Liang Chen and Kuan Feng and Zheng Wang",
year = "2009",
type = "Patent",
}
